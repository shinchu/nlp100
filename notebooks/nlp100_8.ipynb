{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: ニューラルネット\n",
    "\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70. 単語ベクトルの和による特徴量\n",
    "\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$X$と，正解ラベルを並べた行列（ベクトル）$Y$を作成したい．\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  \\boldsymbol{x}_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\n",
    "Y = \\begin{pmatrix} \n",
    "  y_1 \\\\ \n",
    "  y_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  y_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{N}^{n}\n",
    "$$\n",
    "\n",
    "ここで，$n$は学習データの事例数であり，$\\boldsymbol{x}_i \\in \\mathbb{R}^d$と$y_i \\in \\mathbb{N}$はそれぞれ，$i \\in \\{1, \\dots, n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．\n",
    "なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb{N}_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_i \\in \\mathbb{N}_{<4}$で表現できる．\n",
    "以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
    "\n",
    "$i$番目の事例の特徴ベクトル$\\boldsymbol{x}_i$は，次式で求める．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})\n",
    "$$\n",
    "\n",
    "ここで，$i$番目の事例は$T_i$個の（記事見出しの）単語列$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$から構成され，$\\mathrm{emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数は$d$）である．すなわち，$i$番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$\\boldsymbol{x}_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$300$次元の単語ベクトルを用いたので，$d=300$である．\n",
    "\n",
    "$i$番目の事例のラベル$y_i$は，次のように定義する．\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "0 & (\\mbox{記事}x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n",
    "1 & (\\mbox{記事}x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n",
    "2 & (\\mbox{記事}x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n",
    "3 & (\\mbox{記事}x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "\n",
    "+ 学習データの特徴量行列: $X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$\n",
    "+ 学習データのラベルベクトル: $Y_{\\rm train} \\in \\mathbb{N}^{N_t}$\n",
    "+ 検証データの特徴量行列: $X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$\n",
    "+ 検証データのラベルベクトル: $Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$\n",
    "+ 評価データの特徴量行列: $X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$\n",
    "+ 評価データのラベルベクトル: $Y_{\\rm test} \\in \\mathbb{N}^{N_e}$\n",
    "\n",
    "なお，$N_t, N_v, N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('../data/news_aggregator/train.txt')\n",
    "valid_df = pd.read_table('../data/news_aggregator/valid.txt')\n",
    "test_df = pd.read_table('../data/news_aggregator/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['b', 't', 'e', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format('../data/google_news.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, make lowercase and make stem\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(x):\n",
    "    table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    x = x.translate(table).split()\n",
    "    x = [w.lower() for w in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['title'].map(tokenize)\n",
    "valid_df['tokens'] = valid_df['title'].map(tokenize)\n",
    "test_df['tokens'] = test_df['title'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize(x):\n",
    "    return categories.index(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['category'].map(labelize)\n",
    "valid_df['label'] = valid_df['category'].map(labelize)\n",
    "test_df['label'] = test_df['category'].map(labelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vec = np.array([model[w] for w in tokens if w in model])\n",
    "    return torch.tensor(np.mean(vec, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.stack([vectorize(tokens) for tokens in train_df['tokens']])\n",
    "X_valid = torch.stack([vectorize(tokens) for tokens in valid_df['tokens']])\n",
    "X_test = torch.stack([vectorize(tokens) for tokens in test_df['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(train_df['label'])\n",
    "y_valid = torch.tensor(valid_df['label'])\n",
    "y_test = torch.tensor(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(X_train, '../data/X_train.pt')\n",
    "torch.save(X_valid, '../data/X_valid.pt')\n",
    "torch.save(X_test, '../data/X_test.pt')\n",
    "torch.save(y_train, '../data/y_train.pt')\n",
    "torch.save(y_valid, '../data/y_valid.pt')\n",
    "torch.save(y_test, '../data/y_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71. 単層ニューラルネットワークによる予測\n",
    "\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{y}}_1 = {\\rm softmax}(\\boldsymbol{x}_1 W), \\\\\n",
    "\\hat{Y} = {\\rm softmax}(X_{[1:4]} W)\n",
    "$$\n",
    "\n",
    "ただし，${\\rm softmax}$はソフトマックス関数，$X_{[1:4]} \\in \\mathbb{R}^{4 \\times d}$は特徴ベクトル$\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3, \\boldsymbol{x}_4$を縦に並べた行列である．\n",
    "\n",
    "$$\n",
    "X_{[1:4]} = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\boldsymbol{x}_3 \\\\ \n",
    "  \\boldsymbol{x}_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "行列$W \\in \\mathbb{R}^{d \\times L}$は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，$\\hat{\\boldsymbol{y}}_1 \\in \\mathbb{R}^L$は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである．\n",
    "同様に，$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$は，学習データの事例$x_1, x_2, x_3, x_4$について，各カテゴリに属する確率を行列として表現している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load('../data/X_train.pt')\n",
    "X_valid = torch.load('../data/X_valid.pt')\n",
    "X_test = torch.load('../data/X_test.pt')\n",
    "y_train = torch.load('../data/y_train.pt')\n",
    "y_valid = torch.load('../data/y_valid.pt')\n",
    "y_test = torch.load('../data/y_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SLPNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "        nn.init.normal_(self.fc.weight, 0.0, 1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0467, 0.3497, 0.5099, 0.0937]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SLPNet(300, 4)\n",
    "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
    "y_hat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0467, 0.3497, 0.5099, 0.0937],\n",
       "        [0.1622, 0.3983, 0.2954, 0.1442],\n",
       "        [0.1508, 0.0474, 0.1624, 0.6395],\n",
       "        [0.0488, 0.4084, 0.4985, 0.0443]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.softmax(model(X_train[:4]), dim=-1)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72. 損失と勾配の計算\n",
    "\n",
    "学習データの事例$x_1$と事例集合$x_1, x_2, x_3, x_4$に対して，クロスエントロピー損失と，行列$W$に対する勾配を計算せよ．なお，ある事例$x_i$に対して損失は次式で計算される．\n",
    "\n",
    "$$\n",
    "l_i = - \\log [\\mbox{事例}x_i\\mbox{が}y_i\\mbox{に分類される確率}]\n",
    "$$\n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 3.0635852813720703\n",
      "勾配: tensor([[ 0.0291, -0.0393, -0.0438,  ..., -0.0393, -0.0044, -0.0456],\n",
      "        [-0.0107,  0.0144,  0.0160,  ...,  0.0144,  0.0016,  0.0167],\n",
      "        [-0.0156,  0.0210,  0.0234,  ...,  0.0210,  0.0024,  0.0244],\n",
      "        [-0.0029,  0.0039,  0.0043,  ...,  0.0039,  0.0004,  0.0045]])\n"
     ]
    }
   ],
   "source": [
    "loss_1 = criterion(model(X_train[:1]), y_train[:1])\n",
    "model.zero_grad()\n",
    "loss_1.backward()\n",
    "\n",
    "print(f'損失: {loss_1}')\n",
    "print(f'勾配: {model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 2.430011034011841\n",
      "勾配: tensor([[ 0.0139, -0.0235, -0.0299,  ..., -0.0181, -0.0168, -0.0144],\n",
      "        [-0.0091,  0.0140,  0.0132,  ...,  0.0035,  0.0043,  0.0082],\n",
      "        [ 0.0165, -0.0174,  0.0093,  ...,  0.0334,  0.0296, -0.0129],\n",
      "        [-0.0214,  0.0269,  0.0074,  ..., -0.0188, -0.0172,  0.0190]])\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(model(X_train[:4]), y_train[:4])\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "print(f'損失: {loss}')\n",
    "print(f'勾配: {model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73. 確率的勾配降下法による学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列Wを学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "dataset_test = NewsDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SLPNet(300, 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_train: 0.5370, loss_valid: 0.3871\n",
      "epoch: 2, loss_train: 0.3689, loss_valid: 0.3597\n",
      "epoch: 3, loss_train: 0.3374, loss_valid: 0.3456\n",
      "epoch: 4, loss_train: 0.3206, loss_valid: 0.3393\n",
      "epoch: 5, loss_train: 0.3093, loss_valid: 0.3351\n",
      "epoch: 6, loss_train: 0.3007, loss_valid: 0.3340\n",
      "epoch: 7, loss_train: 0.2948, loss_valid: 0.3291\n",
      "epoch: 8, loss_train: 0.2894, loss_valid: 0.3300\n",
      "epoch: 9, loss_train: 0.2856, loss_valid: 0.3285\n",
      "epoch: 10, loss_train: 0.2820, loss_valid: 0.3282\n",
      "epoch: 11, loss_train: 0.2796, loss_valid: 0.3296\n",
      "epoch: 12, loss_train: 0.2774, loss_valid: 0.3293\n",
      "epoch: 13, loss_train: 0.2747, loss_valid: 0.3282\n",
      "epoch: 14, loss_train: 0.2730, loss_valid: 0.3266\n",
      "epoch: 15, loss_train: 0.2715, loss_valid: 0.3286\n",
      "epoch: 16, loss_train: 0.2700, loss_valid: 0.3313\n",
      "epoch: 17, loss_train: 0.2686, loss_valid: 0.3305\n",
      "epoch: 18, loss_train: 0.2669, loss_valid: 0.3322\n",
      "epoch: 19, loss_train: 0.2663, loss_valid: 0.3311\n",
      "epoch: 20, loss_train: 0.2647, loss_valid: 0.3371\n",
      "epoch: 21, loss_train: 0.2650, loss_valid: 0.3319\n",
      "epoch: 22, loss_train: 0.2639, loss_valid: 0.3322\n",
      "epoch: 23, loss_train: 0.2633, loss_valid: 0.3336\n",
      "epoch: 24, loss_train: 0.2619, loss_valid: 0.3347\n",
      "epoch: 25, loss_train: 0.2621, loss_valid: 0.3364\n",
      "epoch: 26, loss_train: 0.2613, loss_valid: 0.3347\n",
      "epoch: 27, loss_train: 0.2609, loss_valid: 0.3421\n",
      "epoch: 28, loss_train: 0.2603, loss_valid: 0.3370\n",
      "epoch: 29, loss_train: 0.2598, loss_valid: 0.3374\n",
      "epoch: 30, loss_train: 0.2595, loss_valid: 0.3375\n",
      "epoch: 31, loss_train: 0.2596, loss_valid: 0.3376\n",
      "epoch: 32, loss_train: 0.2588, loss_valid: 0.3374\n",
      "epoch: 33, loss_train: 0.2584, loss_valid: 0.3387\n",
      "epoch: 34, loss_train: 0.2585, loss_valid: 0.3391\n",
      "epoch: 35, loss_train: 0.2582, loss_valid: 0.3392\n",
      "epoch: 36, loss_train: 0.2575, loss_valid: 0.3403\n",
      "epoch: 37, loss_train: 0.2570, loss_valid: 0.3418\n",
      "epoch: 38, loss_train: 0.2570, loss_valid: 0.3422\n",
      "epoch: 39, loss_train: 0.2563, loss_valid: 0.3441\n",
      "epoch: 40, loss_train: 0.2559, loss_valid: 0.3417\n",
      "epoch: 41, loss_train: 0.2566, loss_valid: 0.3427\n",
      "epoch: 42, loss_train: 0.2558, loss_valid: 0.3451\n",
      "epoch: 43, loss_train: 0.2563, loss_valid: 0.3428\n",
      "epoch: 44, loss_train: 0.2560, loss_valid: 0.3436\n",
      "epoch: 45, loss_train: 0.2555, loss_valid: 0.3450\n",
      "epoch: 46, loss_train: 0.2557, loss_valid: 0.3475\n",
      "epoch: 47, loss_train: 0.2553, loss_valid: 0.3441\n",
      "epoch: 48, loss_train: 0.2548, loss_valid: 0.3467\n",
      "epoch: 49, loss_train: 0.2549, loss_valid: 0.3490\n",
      "epoch: 50, loss_train: 0.2541, loss_valid: 0.3476\n",
      "epoch: 51, loss_train: 0.2543, loss_valid: 0.3471\n",
      "epoch: 52, loss_train: 0.2541, loss_valid: 0.3476\n",
      "epoch: 53, loss_train: 0.2544, loss_valid: 0.3470\n",
      "epoch: 54, loss_train: 0.2540, loss_valid: 0.3485\n",
      "epoch: 55, loss_train: 0.2543, loss_valid: 0.3475\n",
      "epoch: 56, loss_train: 0.2538, loss_valid: 0.3492\n",
      "epoch: 57, loss_train: 0.2542, loss_valid: 0.3489\n",
      "epoch: 58, loss_train: 0.2542, loss_valid: 0.3482\n",
      "epoch: 59, loss_train: 0.2536, loss_valid: 0.3501\n",
      "epoch: 60, loss_train: 0.2536, loss_valid: 0.3488\n",
      "epoch: 61, loss_train: 0.2536, loss_valid: 0.3488\n",
      "epoch: 62, loss_train: 0.2534, loss_valid: 0.3502\n",
      "epoch: 63, loss_train: 0.2533, loss_valid: 0.3522\n",
      "epoch: 64, loss_train: 0.2535, loss_valid: 0.3512\n",
      "epoch: 65, loss_train: 0.2529, loss_valid: 0.3512\n",
      "epoch: 66, loss_train: 0.2527, loss_valid: 0.3511\n",
      "epoch: 67, loss_train: 0.2531, loss_valid: 0.3538\n",
      "epoch: 68, loss_train: 0.2528, loss_valid: 0.3543\n",
      "epoch: 69, loss_train: 0.2528, loss_valid: 0.3514\n",
      "epoch: 70, loss_train: 0.2526, loss_valid: 0.3558\n",
      "epoch: 71, loss_train: 0.2522, loss_valid: 0.3583\n",
      "epoch: 72, loss_train: 0.2513, loss_valid: 0.3524\n",
      "epoch: 73, loss_train: 0.2524, loss_valid: 0.3525\n",
      "epoch: 74, loss_train: 0.2525, loss_valid: 0.3536\n",
      "epoch: 75, loss_train: 0.2529, loss_valid: 0.3540\n",
      "epoch: 76, loss_train: 0.2525, loss_valid: 0.3544\n",
      "epoch: 77, loss_train: 0.2528, loss_valid: 0.3531\n",
      "epoch: 78, loss_train: 0.2526, loss_valid: 0.3545\n",
      "epoch: 79, loss_train: 0.2521, loss_valid: 0.3554\n",
      "epoch: 80, loss_train: 0.2519, loss_valid: 0.3535\n",
      "epoch: 81, loss_train: 0.2520, loss_valid: 0.3537\n",
      "epoch: 82, loss_train: 0.2522, loss_valid: 0.3552\n",
      "epoch: 83, loss_train: 0.2519, loss_valid: 0.3562\n",
      "epoch: 84, loss_train: 0.2514, loss_valid: 0.3564\n",
      "epoch: 85, loss_train: 0.2516, loss_valid: 0.3569\n",
      "epoch: 86, loss_train: 0.2523, loss_valid: 0.3562\n",
      "epoch: 87, loss_train: 0.2507, loss_valid: 0.3674\n",
      "epoch: 88, loss_train: 0.2527, loss_valid: 0.3556\n",
      "epoch: 89, loss_train: 0.2512, loss_valid: 0.3579\n",
      "epoch: 90, loss_train: 0.2522, loss_valid: 0.3561\n",
      "epoch: 91, loss_train: 0.2514, loss_valid: 0.3566\n",
      "epoch: 92, loss_train: 0.2517, loss_valid: 0.3550\n",
      "epoch: 93, loss_train: 0.2513, loss_valid: 0.3571\n",
      "epoch: 94, loss_train: 0.2518, loss_valid: 0.3574\n",
      "epoch: 95, loss_train: 0.2521, loss_valid: 0.3566\n",
      "epoch: 96, loss_train: 0.2519, loss_valid: 0.3574\n",
      "epoch: 97, loss_train: 0.2519, loss_valid: 0.3574\n",
      "epoch: 98, loss_train: 0.2515, loss_valid: 0.3567\n",
      "epoch: 99, loss_train: 0.2516, loss_valid: 0.3581\n",
      "epoch: 100, loss_train: 0.2516, loss_valid: 0.3589\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "    \n",
    "    loss_train = loss_train / i\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        outputs = model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "        \n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74. 正解率の計測\n",
    "\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.916\n",
      "test accuracy: 0.894\n"
     ]
    }
   ],
   "source": [
    "acc_train = calculate_accuracy(model, dataloader_train)\n",
    "acc_test = calculate_accuracy(model, dataloader_test)\n",
    "print(f'train accuracy: {acc_train:.3f}')\n",
    "print(f'test accuracy: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75. 損失と正解率のプロット\n",
    "\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
