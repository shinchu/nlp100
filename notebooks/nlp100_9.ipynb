{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: RNN, CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換\n",
    "\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に1，2番目に頻出する単語に2，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて0とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/nymwa/items/4542b45837a10766890b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['b', 't', 'e', 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_table('../data/news_aggregator/train.txt')\n",
    "valid_df = pd.read_table('../data/news_aggregator/valid.txt')\n",
    "test_df = pd.read_table('../data/news_aggregator/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def tokenize(x):\n",
    "    table = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    x = x.translate(table).split()\n",
    "    x = [w.lower() for w in x]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['title'].map(tokenize)\n",
    "valid_df['tokens'] = valid_df['title'].map(tokenize)\n",
    "test_df['tokens'] = test_df['title'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize(x):\n",
    "    return torch.tensor(categories.index(x), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['category'].map(labelize)\n",
    "valid_df['label'] = valid_df['category'].map(labelize)\n",
    "test_df['label'] = test_df['category'].map(labelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([x for title in train_df['tokens'] for x in title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_train = [token for token, freq in counter.most_common() if freq > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = ['[UNK]'] + vocab_in_train\n",
    "vocab_dict = {x:n for n, x in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2ids(sent):\n",
    "    return torch.tensor([vocab_dict[x if x in vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astrazeneca',\n",
       " 'weighs',\n",
       " 'on',\n",
       " 'europe',\n",
       " 'shares',\n",
       " 'as',\n",
       " 'it',\n",
       " 'rejects',\n",
       " 'pfizer',\n",
       " 'bid']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 368, 1377,    7,  219,   37,    8,   30, 1711,  260,  183])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2ids(train_df['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2ids(x):\n",
    "    return [sent2ids(sent) for sent in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [astrazeneca, weighs, on, europe, shares, as, ...\n",
       "1    [un, officials, want, better, flight, tracking...\n",
       "2    [kim, kardashian, kim, kardashian, visits, wed...\n",
       "3    [update, 1, ackman, asks, for, allergan, s, st...\n",
       "4    [european, car, sales, jump, 7, 6, as, price, ...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tokens'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 368, 1377,    7,  219,   37,    8,   30, 1711,  260,  183]),\n",
       " tensor([ 583,  717,  662,  607,  718, 2252,   18,  283,    5,  718, 1860]),\n",
       " tensor([  33,   27,   33,   27, 1712,  105, 4507]),\n",
       " tensor([   9,   13,  910, 1577,    6,  508,    2,    0,  563]),\n",
       " tensor([ 214,  426,   51,  403,  122,  132,    8,  457,  208,  335, 5702, 2253,\n",
       "           25])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2ids(train_df['tokens'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID番号で表現された単語列$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$がある．ただし，$T$は単語列の長さ，$x_t \\in \\mathbb{R}^{V}$は単語のID番号のone-hot表記である（$V$は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$\\boldsymbol{x}$からカテゴリ$y$を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "+ 単語埋め込みの次元数: $d_w$\n",
    "+ 畳み込みのフィルターのサイズ: 3 トークン\n",
    "+ 畳み込みのストライド: 1 トークン\n",
    "+ 畳み込みのパディング: あり\n",
    "+ 畳み込み演算後の各時刻のベクトルの次元数: $d_h$\n",
    "+ 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$d_h$次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$t$の特徴ベクトル$p_t \\in \\mathbb{R}^{d_h}$は次式で表される．\n",
    "\n",
    "$$\n",
    "p_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)})\n",
    "$$\n",
    "\n",
    "ただし，$W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}$はCNNのパラメータ，$g$は活性化関数（例えば$\\tanh$やReLUなど），$[a; b; c]$はベクトル$a, b, c$の連結である．なお，行列$W^{(px)}$の列数が$3d_w$になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル$c \\in \\mathbb{R}^{d_h}$を求める．$c[i]$でベクトル$c$の$i$番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "$$\n",
    "c[i] = \\max_{1 \\leq t \\leq T} p_t[i]\n",
    "$$\n",
    "\n",
    "最後に，入力文書の特徴ベクトル$c$に行列$W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}$とバイアス項$b^{(y)} \\in \\mathbb{R}^{L}$による線形変換とソフトマックス関数を適用し，カテゴリ$y$を予測する．\n",
    "\n",
    "$$\n",
    "y = {\\rm softmax}(W^{(yc)} c + b^{(y)})\n",
    "$$\n",
    "\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$y$を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNとは\n",
    "\n",
    "参考：[Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "![](https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/convolution_schematic.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/convolutional-neural-network-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 位置普遍性（location invariance）\n",
    "    - 位置を気にしなくてもよい\n",
    "- 構成性（compositionality）\n",
    "    - 低レベルな特徴から高レベルな特徴へ\n",
    "- NLPではn-gramより速い"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://tkengo.github.io/assets/img/understanding-convolutional-neural-networks-for-nlp/convolutional-neural-network-for-nlp-without-pooling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_vocab_list = ['[PAD]', '[UNK]'] + vocab_in_train\n",
    "cnn_vocab_dict = {x:n for n, x in enumerate(cnn_vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnnsent2ids(sent):\n",
    "    return torch.tensor([cnn_vocab_dict[x if x in cnn_vocab_dict else '[UNK]'] for x in sent], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astrazeneca',\n",
       " 'weighs',\n",
       " 'on',\n",
       " 'europe',\n",
       " 'shares',\n",
       " 'as',\n",
       " 'it',\n",
       " 'rejects',\n",
       " 'pfizer',\n",
       " 'bid']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 369, 1378,    8,  220,   38,    9,   31, 1712,  261,  184])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnsent2ids(train_df['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnndf2ids(row):\n",
    "    return [cnnsent2ids(sent) for sent in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = cnndf2ids(train_df['tokens'])\n",
    "x_valid = cnndf2ids(valid_df['tokens'])\n",
    "x_test = cnndf2ids(test_df['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = list(train_df['label'])\n",
    "t_valid = list(valid_df['label'])\n",
    "t_test = list(test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 369, 1378,    8,  220,   38,    9,   31, 1712,  261,  184]),\n",
       " tensor([ 584,  718,  663,  608,  719, 2253,   19,  284,    6,  719, 1861]),\n",
       " tensor([  34,   28,   34,   28, 1713,  106, 4508]),\n",
       " tensor([  10,   14,  911, 1578,    7,  509,    3,    1,  564]),\n",
       " tensor([ 215,  427,   52,  404,  123,  133,    9,  458,  209,  336, 5703, 2254,\n",
       "           26])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNData(Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.lengths = torch.tensor([len(x) for x in source])\n",
    "        self.size = len(source)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'src': self.source[index],\n",
    "            'trg': self.target[index],\n",
    "            'lengths': self.lengths[index]\n",
    "        }\n",
    "        \n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        src = [torch.cat([x['src'], torch.zeros(max_seq_len - x['lengths'], dtype=torch.long)], dim=-1) for x in xs]\n",
    "        src = torch.stack(src)\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return {\n",
    "            'src': src,\n",
    "            'trg': torch.tensor([x['trg'] for x in xs]),\n",
    "            'mask': mask\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, width, shuffle=False):\n",
    "        self.dataset = dataset\n",
    "        self.width = width\n",
    "        self.shuffle = shuffle\n",
    "        if not shuffle:\n",
    "            self.indices = torch.arange(len(dataset))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(len(self.dataset))\n",
    "        index = 0\n",
    "        while index < len(self.dataset):\n",
    "            yield self.indices[index:index+self.width]\n",
    "            index += self.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DescendingSampler(Sampler):\n",
    "    def __init__(self, dataset, width, shuffle=False):\n",
    "        assert not shuffle\n",
    "        super().__init__(dataset, width, shuffle)\n",
    "        self.indices = self.indices[self.dataset.lengths[self.indices].argsort(descending=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxTokensSampler(Sampler):\n",
    "    def __iter__(self):\n",
    "        self.indices = torch.randperm(len(self.dataset))\n",
    "        self.indices = self.indices[self.dataset.lengths[self.indices].argsort(descending=True)]\n",
    "        for batch in self.generate_batches():\n",
    "            yield batch\n",
    "\n",
    "    def generate_batches(self):\n",
    "        batches = []\n",
    "        batch = []\n",
    "        acc = 0\n",
    "        max_len = 0\n",
    "        for index in self.indices:\n",
    "            acc += 1\n",
    "            this_len = self.dataset.lengths[index]\n",
    "            max_len = max(max_len, this_len)\n",
    "            if acc * max_len > self.width:\n",
    "                batches.append(batch)\n",
    "                batch = [index]\n",
    "                acc = 1\n",
    "                max_len = this_len\n",
    "            else:\n",
    "                batch.append(index)\n",
    "        if batch != []:\n",
    "            batches.append(batch)\n",
    "        random.shuffle(batches)\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_loader(dataset, width, sampler=Sampler, shuffle=False, num_workers=8):\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_sampler = sampler(dataset, width, shuffle),\n",
    "        collate_fn = dataset.collate,\n",
    "        num_workers = num_workers,\n",
    "    )\n",
    "\n",
    "def gen_descending_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = DescendingSampler, shuffle = False, num_workers = num_workers)\n",
    "\n",
    "def gen_maxtokens_loader(dataset, width, num_workers=0):\n",
    "    return gen_loader(dataset, width, sampler = MaxTokensSampler, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CNNData(x_train, t_train)\n",
    "valid_dataset = CNNData(x_valid, t_valid)\n",
    "test_dataset = CNNData(x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, v_size, e_size, h_size, c_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(v_size, e_size)\n",
    "        self.conv = nn.Conv1d(e_size, h_size, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(h_size, c_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x = self.embed(batch['src'])\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv(x.transpose(-1, -2))\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x.masked_fill_(batch['mask'].unsqueeze(-2) == 0, -1e4)\n",
    "        x = F.max_pool1d(x, x.size(-1)).squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = gen_loader(test_dataset, 10, DescendingSampler, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(len(cnn_vocab_list), 300, 128, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 3, 2, 0, 2])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(iter(loader).next()).argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self):\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_step(self, model, batch):\n",
    "        model.zero_grad()\n",
    "        loss = self.criterion(model(batch), batch['trg'])\n",
    "        loss.backward()\n",
    "        return loss.item()\n",
    "\n",
    "    def valid_step(self, model, batch):\n",
    "        with torch.no_grad():\n",
    "            loss = self.criterion(model(batch), batch['trg'])\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loaders, task, optimizer, max_iter, device = None):\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.train_loader, self.valid_loader = loaders\n",
    "        self.task = task\n",
    "        self.optimizer = optimizer\n",
    "        self.max_iter = max_iter\n",
    "        self.device = device\n",
    "\n",
    "    def send(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        acc = 0\n",
    "        for n, batch in enumerate(self.train_loader):\n",
    "            batch = self.send(batch)\n",
    "            acc += self.task.train_step(self.model, batch)\n",
    "            self.optimizer.step()\n",
    "        return acc / n\n",
    "\n",
    "    def valid_epoch(self):\n",
    "        self.model.eval()\n",
    "        acc = 0\n",
    "        for n, batch in enumerate(self.valid_loader):\n",
    "            batch = self.send(batch)\n",
    "            acc += self.task.valid_step(self.model, batch)\n",
    "        return acc / n\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.max_iter):\n",
    "            train_loss = self.train_epoch()\n",
    "            valid_loss = self.valid_epoch()\n",
    "            print('epoch {}, train_loss:{:.5f}, valid_loss:{:.5f}'.format(epoch, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, loader, device=None):\n",
    "        self.model = model\n",
    "        self.loader = loader\n",
    "        self.device = device\n",
    "\n",
    "    def send(self, batch):\n",
    "        for key in batch:\n",
    "            batch[key] = batch[key].to(self.device)\n",
    "        return batch\n",
    "\n",
    "    def infer(self, batch):\n",
    "        self.model.eval()\n",
    "        batch = self.send(batch)\n",
    "        return self.model(batch).argmax(dim=-1).item()\n",
    "\n",
    "    def predict(self):\n",
    "        lst = []\n",
    "        for batch in self.loader:\n",
    "            lst.append(self.infer(batch))\n",
    "        return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors \n",
    "vectors = KeyedVectors.load_word2vec_format('../data/google_news.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn_embed(embed):\n",
    "    for i, token in enumerate(cnn_vocab_list):\n",
    "        if token in vectors:\n",
    "            embed.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(true, pred):\n",
    "    return np.mean([t == p for t, p in zip(true, pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:1.21804, valid_loss:1.10892\n",
      "epoch 1, train_loss:1.05988, valid_loss:0.94881\n",
      "epoch 2, train_loss:0.88027, valid_loss:0.80689\n",
      "epoch 3, train_loss:0.74920, valid_loss:0.72029\n",
      "epoch 4, train_loss:0.66906, valid_loss:0.66293\n",
      "epoch 5, train_loss:0.62509, valid_loss:0.61697\n",
      "epoch 6, train_loss:0.57443, valid_loss:0.59757\n",
      "epoch 7, train_loss:0.54502, valid_loss:0.57207\n",
      "epoch 8, train_loss:0.52024, valid_loss:0.55041\n",
      "epoch 9, train_loss:0.49358, valid_loss:0.51679\n",
      "epoch 10, train_loss:0.46825, valid_loss:0.49555\n",
      "epoch 11, train_loss:0.44289, valid_loss:0.47474\n",
      "epoch 12, train_loss:0.41912, valid_loss:0.46023\n",
      "epoch 13, train_loss:0.39720, valid_loss:0.44621\n",
      "epoch 14, train_loss:0.36676, valid_loss:0.42989\n",
      "epoch 15, train_loss:0.37013, valid_loss:0.42167\n",
      "epoch 16, train_loss:0.35534, valid_loss:0.41590\n",
      "epoch 17, train_loss:0.33199, valid_loss:0.40327\n",
      "epoch 18, train_loss:0.31758, valid_loss:0.39645\n",
      "epoch 19, train_loss:0.31328, valid_loss:0.39320\n"
     ]
    }
   ],
   "source": [
    "model = CNN(len(cnn_vocab_dict), 300, 128, 4)\n",
    "init_cnn_embed(model.embed)\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(train_dataset, 4000),\n",
    "    gen_descending_loader(valid_dataset, 32),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 20, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.9199775112443778\n",
      "test accuracy 0.889055472263868\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('train accuracy', accuracy(t_train, pred))\n",
    "\n",
    "predictor = Predictor(model, gen_loader(test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('test accuracy', accuracy(t_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bert(x):\n",
    "    return torch.tensor(tokenizer.encode(x, max_length=512, truncation=True), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(train_df['title'].map(tokenize_bert))\n",
    "x_valid = list(valid_df['title'].map(tokenize_bert))\n",
    "x_test = list(test_df['title'].map(tokenize_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(CNNData):\n",
    "    def collate(self, xs):\n",
    "        max_seq_len = max([x['lengths'] for x in xs])\n",
    "        src = [torch.cat([x['src'], torch.zeros(max_seq_len - x['lengths'], dtype=torch.long)], dim=-1) for x in xs]\n",
    "        src = torch.stack(src)\n",
    "        mask = [[1] * x['lengths'] + [0] * (max_seq_len - x['lengths']) for x in xs]\n",
    "        mask = torch.tensor(mask, dtype=torch.long)\n",
    "        return {\n",
    "            'src':src,\n",
    "            'trg':torch.tensor([x['trg'] for x in xs]),\n",
    "            'mask':mask,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train_dataset = BertDataset(x_train, t_train)\n",
    "bert_valid_dataset = BertDataset(x_valid, t_valid)\n",
    "bert_test_dataset = BertDataset(x_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        config = BertConfig.from_pretrained('bert-base-cased', num_labels=4)\n",
    "        self.bert = BertForSequenceClassification.from_pretrained('bert-base-cased', config=config)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.bert(batch['src'], attention_mask=batch['mask'])\n",
    "        return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train_loss:0.59589, valid_loss:0.31145\n",
      "epoch 1, train_loss:0.22326, valid_loss:0.25254\n",
      "epoch 2, train_loss:0.14306, valid_loss:0.23172\n",
      "epoch 3, train_loss:0.09717, valid_loss:0.24095\n",
      "epoch 4, train_loss:0.06437, valid_loss:0.26058\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "loaders = (\n",
    "    gen_maxtokens_loader(bert_train_dataset, 1000),\n",
    "    gen_descending_loader(bert_valid_dataset, 32),\n",
    ")\n",
    "task = Task()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "trainer = Trainer(model, loaders, task, optimizer, 5, device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 0.991191904047976\n",
      "test accuracy 0.952023988005997\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, gen_loader(bert_train_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('train accuracy', accuracy(t_train, pred))\n",
    "\n",
    "predictor = Predictor(model, gen_loader(bert_test_dataset, 1), device)\n",
    "pred = predictor.predict()\n",
    "print('test accuracy', accuracy(t_test, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
